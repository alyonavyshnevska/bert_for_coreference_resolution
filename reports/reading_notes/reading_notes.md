## Bert for Coreference Resolution
Joshi et al 2019


## What do we learn from context? Probing for sentence structure in contextualized word represenattions 

Tenney et al 2019


## The Illustrated Transformer (blog post)

Jay Allamar

1. Stack of 6 enoders, stack of 6 decoders 
2. Simplified verison with 2:

![](graphics/transformer.png)

3. Multi-headed self-attention: 
    - useful if we’re translating a sentence like “The animal didn’t cross the street because it was too tired”. Which word “it” refers to.


![](graphics/transformer_multi-headed_self-attention-recap.png)

4. [Blogpost](http://jalammar.github.io/illustrated-transformer/)
